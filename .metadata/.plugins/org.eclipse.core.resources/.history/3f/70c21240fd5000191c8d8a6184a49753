'''
Created on Mar 27, 2019

@author: LiuYuhao
'''

''' ROADMAP
Input all files and store in separate classes
Preprocessing
Tokenization
Build bag-of-words representation of each documents using collection.counter
tf-idf
'''
import os
import array as arr
# import subprocess as sh
import collections.Counter as ct, corpus

class vocab():
    def __init__(self):
        print()
        
def main():
    #method variables
    docs = arr.array()
    stopwords = ["a", "the", "of", "with"]
    os.chdir('news')
    index = 0;
    
    while index <= 511:
        # open file
        file = open("{:03d}.txt", index)
        lines = file.read().lower() # convert all to lowercase; type = str
        # tokenize
        tokens = lines.split()
        # pre-processing
        for token in tokens:
            #remove stop words
            if stopwords.__contains__(token):
                tokens.remove(token)
            # build BoW for this file
            ct[token] += 1
        docs.append(ct)
        ct.clear()
        file.close()
        index+=1
        
    # combine all docs to obtain dictionary
    dict["a"] = 1
    for doc in docs:
        print("a")
        

if __name__ == '__main__':
    main()